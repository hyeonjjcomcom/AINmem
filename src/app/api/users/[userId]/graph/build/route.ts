import { NextRequest, NextResponse } from 'next/server';
import {
  FolBuilder,
  GeminiAdapter,
  createFolClient
} from 'fol-sdk';
import mongoose from 'mongoose';
import connectDB from '@/lib/mongodb';
import { getFolStore } from '@/lib/folStore';
import ChatLog from '@/models/chatLogs';
import BuildHistory from '@/models/buildHistory';

/**
 * POST /api/users/[userId]/graph/build
 *
 * Incremental build: build_atì´ ì—†ëŠ” chatlogë“¤ë§Œ ê°€ì ¸ì™€ì„œ FOL ë¹Œë“œ
 *
 * ì²­í¬ ê¸°ë°˜ ë¹Œë“œ ì „ëµ:
 * 1. build_atì´ ì—†ëŠ” chatlogë“¤ì„ ê°€ì ¸ì˜¨ë‹¤
 * 2. ë©”ì‹œì§€ 10ê°œ ë˜ëŠ” 10000 í† í° ë‹¨ìœ„ë¡œ ì²­í¬ë¥¼ ë§Œë“ ë‹¤
 * 3. ê° ì²­í¬ë¥¼ FOL buildí•˜ê³  ì €ì¥
 * 4. ì„±ê³µí•œ ì²­í¬ì˜ ë©”ëª¨ë¦¬ë“¤ë§Œ build_at ì—…ë°ì´íŠ¸ (_id ê¸°ë°˜)
 * 5. ì‹¤íŒ¨í•œ ì²­í¬ëŠ” build_atì´ ì—†ì–´ì„œ ë‹¤ìŒ ë¹Œë“œì—ì„œ ìë™ ì¬ì²˜ë¦¬ë¨
 */

// í† í° ìˆ˜ ì¶”ì • í•¨ìˆ˜ (token_count í•„ë“œê°€ ì—†ì„ ê²½ìš° ì‚¬ìš©)
function estimateTokens(text: string): number {
  // ê°„ë‹¨í•œ ì¶”ì •: í‰ê· ì ìœ¼ë¡œ 1 í† í° â‰ˆ 4 ê¸€ì
  return Math.ceil(text.length / 4);
}

// ì²­í¬ ì¸í„°í˜ì´ìŠ¤
interface Chunk {
  memories: any[];
  totalTokens: number;
  memoryIds: mongoose.Types.ObjectId[];
  document: string;
}

// ì²­í¬ ìƒì„± í•¨ìˆ˜
function createChunks(
  unbuildMemories: any[],
  chunkSize: number,
  maxTokens: number
): Chunk[] {
  const chunks: Chunk[] = [];
  let currentChunk: any[] = [];
  let currentTokens = 0;

  for (const memory of unbuildMemories) {
    const tokens = memory.token_count ?? estimateTokens(memory.input_text);

    // ì²­í¬ê°€ ì´ë¯¸ ìˆê³ , ì¶”ê°€ ì‹œ ì œí•œ ì´ˆê³¼í•˜ë©´ ì²­í¬ ì™„ì„±
    if (currentChunk.length > 0 &&
        (currentChunk.length >= chunkSize ||
         currentTokens + tokens > maxTokens)) {
      chunks.push({
        memories: currentChunk,
        totalTokens: currentTokens,
        memoryIds: currentChunk.map(m => m._id),
        document: currentChunk.map(m => m.input_text).join(' ').trim()
      });
      currentChunk = [];
      currentTokens = 0;
    }

    currentChunk.push(memory);
    currentTokens += tokens;
  }

  // ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
  if (currentChunk.length > 0) {
    chunks.push({
      memories: currentChunk,
      totalTokens: currentTokens,
      memoryIds: currentChunk.map(m => m._id),
      document: currentChunk.map(m => m.input_text).join(' ').trim()
    });
  }

  return chunks;
}

// ì²­í¬ ë¹Œë“œ í•¨ìˆ˜
async function buildChunk(
  chunk: Chunk,
  userId: string,
  client: any
): Promise<{ success: boolean; error?: string }> {
  const buildStartTime = Date.now();

  try {
    // ë¹Œë“œ ì „ FOL ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    const [constantsBeforeList, factsBeforeList, predicatesBeforeList] =
      await Promise.all([
        mongoose.connection.collection('constants')
          .find({ user_id: userId }).project({ _id: 1 }).toArray(),
        mongoose.connection.collection('facts')
          .find({ user_id: userId }).project({ _id: 1 }).toArray(),
        mongoose.connection.collection('predicates')
          .find({ user_id: userId }).project({ _id: 1 }).toArray()
      ]);

    const constantIdsBefore = new Set(constantsBeforeList.map(c => c._id.toString()));
    const factIdsBefore = new Set(factsBeforeList.map(f => f._id.toString()));
    const predicateIdsBefore = new Set(predicatesBeforeList.map(p => p._id.toString()));

    // FOL ë¹Œë“œ ë° ì €ì¥
    await client.buildAndSave(chunk.document, userId);

    // ë¹Œë“œ í›„ FOL ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    const [constantsAfterList, factsAfterList, predicatesAfterList] =
      await Promise.all([
        mongoose.connection.collection('constants')
          .find({ user_id: userId }).project({ _id: 1 }).toArray(),
        mongoose.connection.collection('facts')
          .find({ user_id: userId }).project({ _id: 1 }).toArray(),
        mongoose.connection.collection('predicates')
          .find({ user_id: userId }).project({ _id: 1 }).toArray()
      ]);

    // ìƒˆë¡œ ìƒì„±ëœ FOL ID í•„í„°ë§
    const newConstantIds = constantsAfterList
      .filter(c => !constantIdsBefore.has(c._id.toString()))
      .map(c => c._id);
    const newFactIds = factsAfterList
      .filter(f => !factIdsBefore.has(f._id.toString()))
      .map(f => f._id);
    const newPredicateIds = predicatesAfterList
      .filter(p => !predicateIdsBefore.has(p._id.toString()))
      .map(p => p._id);

    const buildDuration = Date.now() - buildStartTime;

    // BuildHistory ì €ì¥ (chunk_index ì œê±°)
    await BuildHistory.create({
      user_id: userId,
      document: chunk.document,
      memory_ids: chunk.memoryIds,
      token_count: chunk.totalTokens,
      message_count: chunk.memories.length,
      build_type: 'incremental',
      status: 'success',
      generated_constants_count: newConstantIds.length,
      generated_facts_count: newFactIds.length,
      generated_predicates_count: newPredicateIds.length,
      generated_constant_ids: newConstantIds,
      generated_fact_ids: newFactIds,
      generated_predicate_ids: newPredicateIds,
      build_duration_ms: buildDuration
    });

    // ì„±ê³µí•œ ì²­í¬ì˜ ë©”ëª¨ë¦¬ë“¤ë§Œ build_at ì—…ë°ì´íŠ¸
    await ChatLog.updateMany(
      { _id: { $in: chunk.memoryIds } },
      { $set: { build_at: new Date() } }
    );

    console.log(`âœ… Chunk built successfully: ${chunk.memories.length} memories`);
    return { success: true };
  } catch (error: any) {
    console.error(`âŒ Error building chunk:`, error.message);

    // ì‹¤íŒ¨í•œ ì²­í¬ë„ BuildHistoryì— ê¸°ë¡
    try {
      await BuildHistory.create({
        user_id: userId,
        document: chunk.document,
        memory_ids: chunk.memoryIds,
        token_count: chunk.totalTokens,
        message_count: chunk.memories.length,
        build_type: 'incremental',
        status: 'failed',
        error_message: error.message,
        generated_constants_count: 0,
        generated_facts_count: 0,
        generated_predicates_count: 0,
        generated_constant_ids: [],
        generated_fact_ids: [],
        generated_predicate_ids: [],
        build_duration_ms: Date.now() - buildStartTime
      });
    } catch (historyError) {
      console.error(`âŒ Error saving build history:`, historyError);
    }

    // ì‹¤íŒ¨í•œ ì²­í¬ëŠ” build_atì´ ì—†ìœ¼ë¯€ë¡œ ë‹¤ìŒ ë¹Œë“œì—ì„œ ìë™ ì¬ì²˜ë¦¬ë¨
    console.log(`âš ï¸ ${chunk.memories.length} memories will be retried in next build`);
    return { success: false, error: error.message };
  }
}

export async function POST(
  request: NextRequest,
  { params }: { params: Promise<{ userId: string }> }
) {
  try {
    await connectDB();
    const { userId } = await params;

    if (!userId) {
      return NextResponse.json(
        { success: false, error: 'userId is required' },
        { status: 400 }
      );
    }

    console.log('ğŸ”§ Starting incremental build for user:', userId);

    // Step 1: build_atì´ ì—†ëŠ” ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
    const unbuildMemories = await ChatLog.find({
      user_id: userId,
      build_at: { $exists: false }
    }).sort({ createdAt: 1 }); // ì‹œê°„ìˆœ ì •ë ¬

    // ë¹Œë“œí•  ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    if (unbuildMemories.length === 0) {
      console.log('ğŸ“Š No new memories to build');
      return NextResponse.json({
        success: true,
        message: 'No new memories to build',
        builtMemories: 0
      });
    }

    console.log(`ğŸ“¦ Found ${unbuildMemories.length} unbuilt memories`);

    // Step 2: FOL ë¹Œë“œ ì„¤ì •
    const geminiApiKey = process.env.GEMINI_API_KEY;
    if (!geminiApiKey) {
      return NextResponse.json(
        { success: false, error: 'GEMINI_API_KEY not configured' },
        { status: 500 }
      );
    }

    const llmAdapter = new GeminiAdapter(geminiApiKey);
    const store = getFolStore();
    const builder = new FolBuilder({ llm: llmAdapter });
    const client = createFolClient(builder, store);

    // Step 3: ì²­í¬ ìƒì„±
    const CHUNK_SIZE = 10;
    const PREFERRED_MAX_TOKENS = 10000;
    const chunks = createChunks(unbuildMemories, CHUNK_SIZE, PREFERRED_MAX_TOKENS);

    console.log(`ğŸ“¦ Created ${chunks.length} chunks`);

    // Step 4: ì œí•œì  ë³‘ë ¬ ì²˜ë¦¬ (3ê°œì”©)
    const CONCURRENT_LIMIT = 3;
    let totalBuiltMemories = 0;
    let successfulChunks = 0;
    let failedChunks = 0;

    for (let i = 0; i < chunks.length; i += CONCURRENT_LIMIT) {
      const batch = chunks.slice(i, i + CONCURRENT_LIMIT);
      console.log(`ğŸ”„ Processing batch ${Math.floor(i / CONCURRENT_LIMIT) + 1}: ${batch.length} chunks`);

      // ë°°ì¹˜ ë‚´ ì²­í¬ë“¤ì„ ë³‘ë ¬ ì²˜ë¦¬
      const results = await Promise.allSettled(
        batch.map(chunk => buildChunk(chunk, userId, client))
      );

      // ê²°ê³¼ ì§‘ê³„
      for (let j = 0; j < results.length; j++) {
        const result = results[j];
        const chunk = batch[j];

        if (result.status === 'fulfilled' && result.value.success) {
          successfulChunks++;
          totalBuiltMemories += chunk.memories.length;
        } else {
          failedChunks++;
          const errorMsg = result.status === 'rejected'
            ? result.reason?.message
            : result.value.error;
          console.log(`âŒ Chunk failed: ${errorMsg}`);
        }
      }
    }

    console.log(`âœ… Build complete: ${successfulChunks} success, ${failedChunks} failed`);

    return NextResponse.json({
      success: true,
      message: 'Graph built successfully',
      builtMemories: totalBuiltMemories,
      totalChunks: chunks.length,
      successfulChunks,
      failedChunks
    });
  } catch (error: any) {
    console.error('âŒ Error building graph:', error);
    return NextResponse.json(
      { success: false, error: error.message },
      { status: 500 }
    );
  }
}
